# llm-server-benchmark
Simple latency benchmark to run against any OpenAI compatible server
